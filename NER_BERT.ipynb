{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DX0rUG1SuD8G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "2tXrx7uYXJtB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import BertTokenizerFast, DataCollatorForTokenClassification, AutoModelForTokenClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import pipeline\n",
        "import json\n",
        "import datasets\n",
        "import evaluate\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "zStGHg56ZHv1"
      },
      "outputs": [],
      "source": [
        "conll2003 = datasets.load_dataset('conll2003')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab2eN9iwXUoQ",
        "outputId": "2231a310-f6a2-4e74-a658-68da58f77577"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conll2003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "EW00_SbyzyYP",
        "outputId": "3aec55cc-790b-432f-9a10-2af91cef67e0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\\nnot belong to the previous three groups.\\n\\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\\ntagging scheme, whereas the original dataset uses IOB1.\\n\\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\\n'"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conll2003['train'].description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKyFRicjzGUF",
        "outputId": "a321e0ee-e622-4d96-e9d4-6ff437c69ed6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '1',\n",
              " 'tokens': ['Peter', 'Blackburn'],\n",
              " 'pos_tags': [22, 22],\n",
              " 'chunk_tags': [11, 12],\n",
              " 'ner_tags': [1, 2]}"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conll2003[\"train\"][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di3sTF2OxcEo",
        "outputId": "f4677087-2845-46c1-9878-e49eaf54628d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(14041, 5)\n",
            "(3250, 5)\n",
            "(3453, 5)\n"
          ]
        }
      ],
      "source": [
        "print(conll2003[\"train\"].shape)\n",
        "print(conll2003[\"validation\"].shape)\n",
        "print(conll2003[\"test\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YC3PBNZxcHq",
        "outputId": "d753c6d4-77c4-48fc-efc7-8a6d0e368c51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None)"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conll2003[\"train\"].features['pos_tags']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvq8iezwxcLN",
        "outputId": "f57d9508-5897-4659-a42c-da27a3e753fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None)"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conll2003[\"train\"].features['chunk_tags']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGpjacjxzkhf",
        "outputId": "3ace84b4-bbfc-45a2-bee3-ff3b7aaae14f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conll2003[\"train\"].features['ner_tags']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "YhywlBncXUrf"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcH9GmEk2fvp",
        "outputId": "a57bba7d-85fb-4669-ddd1-f699e2d85620"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '5',\n",
              " 'tokens': ['\"',\n",
              "  'We',\n",
              "  'do',\n",
              "  \"n't\",\n",
              "  'support',\n",
              "  'any',\n",
              "  'such',\n",
              "  'recommendation',\n",
              "  'because',\n",
              "  'we',\n",
              "  'do',\n",
              "  \"n't\",\n",
              "  'see',\n",
              "  'any',\n",
              "  'grounds',\n",
              "  'for',\n",
              "  'it',\n",
              "  ',',\n",
              "  '\"',\n",
              "  'the',\n",
              "  'Commission',\n",
              "  \"'s\",\n",
              "  'chief',\n",
              "  'spokesman',\n",
              "  'Nikolaus',\n",
              "  'van',\n",
              "  'der',\n",
              "  'Pas',\n",
              "  'told',\n",
              "  'a',\n",
              "  'news',\n",
              "  'briefing',\n",
              "  '.'],\n",
              " 'pos_tags': [0,\n",
              "  28,\n",
              "  41,\n",
              "  30,\n",
              "  37,\n",
              "  12,\n",
              "  16,\n",
              "  21,\n",
              "  15,\n",
              "  28,\n",
              "  41,\n",
              "  30,\n",
              "  37,\n",
              "  12,\n",
              "  24,\n",
              "  15,\n",
              "  28,\n",
              "  6,\n",
              "  0,\n",
              "  12,\n",
              "  22,\n",
              "  27,\n",
              "  16,\n",
              "  21,\n",
              "  22,\n",
              "  22,\n",
              "  14,\n",
              "  22,\n",
              "  38,\n",
              "  12,\n",
              "  21,\n",
              "  21,\n",
              "  7],\n",
              " 'chunk_tags': [0,\n",
              "  11,\n",
              "  21,\n",
              "  22,\n",
              "  22,\n",
              "  11,\n",
              "  12,\n",
              "  12,\n",
              "  17,\n",
              "  11,\n",
              "  21,\n",
              "  22,\n",
              "  22,\n",
              "  11,\n",
              "  12,\n",
              "  13,\n",
              "  11,\n",
              "  0,\n",
              "  0,\n",
              "  11,\n",
              "  12,\n",
              "  11,\n",
              "  12,\n",
              "  12,\n",
              "  12,\n",
              "  12,\n",
              "  12,\n",
              "  12,\n",
              "  21,\n",
              "  11,\n",
              "  12,\n",
              "  12,\n",
              "  0],\n",
              " 'ner_tags': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  3,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  2,\n",
              "  2,\n",
              "  2,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conll2003[\"train\"][5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XNrjBz9-XUv1",
        "outputId": "f59bde07-fcf8-4f9a-f290-dd3178809de3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1000, 2057, 2079, 1050, 1005, 1056, 2490, 2151, 2107, 12832, 2138, 2057, 2079, 1050, 1005, 1056, 2156, 2151, 5286, 2005, 2009, 1010, 1000, 1996, 3222, 1005, 1055, 2708, 14056, 24794, 2271, 3158, 4315, 14674, 2409, 1037, 2739, 27918, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_text = conll2003[\"train\"][5]\n",
        "tokenized_inputs = tokenizer(example_text[\"tokens\"], is_split_into_words=True)\n",
        "tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghD63KD8XUzw",
        "outputId": "f2811836-53f3-4e93-8cb0-e803c164ae56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', '\"', 'we', 'do', 'n', \"'\", 't', 'support', 'any', 'such', 'recommendation', 'because', 'we', 'do', 'n', \"'\", 't', 'see', 'any', 'grounds', 'for', 'it', ',', '\"', 'the', 'commission', \"'\", 's', 'chief', 'spokesman', 'nikola', '##us', 'van', 'der', 'pas', 'told', 'a', 'news', 'briefing', '.', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biYMcNWIXU3m",
        "outputId": "2e07e13f-7ded-4702-c9b7-38f5ca505611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 21, 22, 23, 24, 24, 25, 26, 27, 28, 29, 30, 31, 32, None]\n"
          ]
        }
      ],
      "source": [
        "word_ids = tokenized_inputs.word_ids()\n",
        "print(word_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4noiTJg6TPI",
        "outputId": "72f34221-ee12-4fa5-985d-af322932d601"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdVHRfvM7PLo",
        "outputId": "599ced98-7477-4ced-e820-e36f6354d766"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(conll2003[\"train\"][5][\"ner_tags\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpKzqC257POT",
        "outputId": "4ac6a881-6fdc-4958-e953-af0851711811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(conll2003[\"train\"][5][\"ner_tags\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "3H7_FYCY6TSM"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        # word_ids() => Return a list mapping the tokens\n",
        "        # to their actual word in the initial sentence.\n",
        "        # It Returns a list indicating the word corresponding to each token.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        # Special tokens like `` and `<\\s>` are originally mapped to None\n",
        "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # set –100 as the label for these special tokens\n",
        "                label_ids.append(-100)\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # if current word_idx is != prev then its the most regular case\n",
        "                # and add the corresponding token\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                # to take care of sub-words which have the same word_idx\n",
        "                # set -100 as well for them, but only if label_all_tokens == False\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "                # mask the subword representations after the first subword\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4BD1Q6m9hdc",
        "outputId": "71bb5e6b-4547-4bbd-bb27-aaedd2a306a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '4',\n",
              " 'tokens': ['Germany',\n",
              "  \"'s\",\n",
              "  'representative',\n",
              "  'to',\n",
              "  'the',\n",
              "  'European',\n",
              "  'Union',\n",
              "  \"'s\",\n",
              "  'veterinary',\n",
              "  'committee',\n",
              "  'Werner',\n",
              "  'Zwingmann',\n",
              "  'said',\n",
              "  'on',\n",
              "  'Wednesday',\n",
              "  'consumers',\n",
              "  'should',\n",
              "  'buy',\n",
              "  'sheepmeat',\n",
              "  'from',\n",
              "  'countries',\n",
              "  'other',\n",
              "  'than',\n",
              "  'Britain',\n",
              "  'until',\n",
              "  'the',\n",
              "  'scientific',\n",
              "  'advice',\n",
              "  'was',\n",
              "  'clearer',\n",
              "  '.'],\n",
              " 'pos_tags': [22,\n",
              "  27,\n",
              "  21,\n",
              "  35,\n",
              "  12,\n",
              "  22,\n",
              "  22,\n",
              "  27,\n",
              "  16,\n",
              "  21,\n",
              "  22,\n",
              "  22,\n",
              "  38,\n",
              "  15,\n",
              "  22,\n",
              "  24,\n",
              "  20,\n",
              "  37,\n",
              "  21,\n",
              "  15,\n",
              "  24,\n",
              "  16,\n",
              "  15,\n",
              "  22,\n",
              "  15,\n",
              "  12,\n",
              "  16,\n",
              "  21,\n",
              "  38,\n",
              "  17,\n",
              "  7],\n",
              " 'chunk_tags': [11,\n",
              "  11,\n",
              "  12,\n",
              "  13,\n",
              "  11,\n",
              "  12,\n",
              "  12,\n",
              "  11,\n",
              "  12,\n",
              "  12,\n",
              "  12,\n",
              "  12,\n",
              "  21,\n",
              "  13,\n",
              "  11,\n",
              "  12,\n",
              "  21,\n",
              "  22,\n",
              "  11,\n",
              "  13,\n",
              "  11,\n",
              "  1,\n",
              "  13,\n",
              "  11,\n",
              "  17,\n",
              "  11,\n",
              "  12,\n",
              "  12,\n",
              "  21,\n",
              "  1,\n",
              "  0],\n",
              " 'ner_tags': [5,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  3,\n",
              "  4,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  2,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  5,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conll2003['train'][4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrLye_oT9hgq",
        "outputId": "43562206-e3f9-4963-d2df-73f59084a6ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}\n"
          ]
        }
      ],
      "source": [
        "exp = tokenize_and_align_labels(conll2003['train'][4:5])\n",
        "print(exp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1fu7bBk9hnI",
        "outputId": "980cf801-c9c6-43ec-b712-ad07129f4536"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS]___________________________________ -100\n",
            "germany_________________________________ 5\n",
            "'_______________________________________ 0\n",
            "s_______________________________________ 0\n",
            "representative__________________________ 0\n",
            "to______________________________________ 0\n",
            "the_____________________________________ 0\n",
            "european________________________________ 3\n",
            "union___________________________________ 4\n",
            "'_______________________________________ 0\n",
            "s_______________________________________ 0\n",
            "veterinary______________________________ 0\n",
            "committee_______________________________ 0\n",
            "werner__________________________________ 1\n",
            "z_______________________________________ 2\n",
            "##wing__________________________________ 2\n",
            "##mann__________________________________ 2\n",
            "said____________________________________ 0\n",
            "on______________________________________ 0\n",
            "wednesday_______________________________ 0\n",
            "consumers_______________________________ 0\n",
            "should__________________________________ 0\n",
            "buy_____________________________________ 0\n",
            "sheep___________________________________ 0\n",
            "##me____________________________________ 0\n",
            "##at____________________________________ 0\n",
            "from____________________________________ 0\n",
            "countries_______________________________ 0\n",
            "other___________________________________ 0\n",
            "than____________________________________ 0\n",
            "britain_________________________________ 5\n",
            "until___________________________________ 0\n",
            "the_____________________________________ 0\n",
            "scientific______________________________ 0\n",
            "advice__________________________________ 0\n",
            "was_____________________________________ 0\n",
            "clearer_________________________________ 0\n",
            "._______________________________________ 0\n",
            "[SEP]___________________________________ -100\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(exp[\"input_ids\"][0]),exp[\"labels\"][0]):\n",
        "    print(f\"{token:_<40} {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "JxEmSLmk_lNn"
      },
      "outputs": [],
      "source": [
        "# applying on entire dataset\n",
        "tokenized_datasets = conll2003.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhi_lGTx_lQb",
        "outputId": "a6926327-2df3-4c65-bd7c-3d64dda2a1cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '1',\n",
              " 'tokens': ['Peter', 'Blackburn'],\n",
              " 'pos_tags': [22, 22],\n",
              " 'chunk_tags': [11, 12],\n",
              " 'ner_tags': [1, 2],\n",
              " 'input_ids': [101, 2848, 13934, 102],\n",
              " 'token_type_ids': [0, 0, 0, 0],\n",
              " 'attention_mask': [1, 1, 1, 1],\n",
              " 'labels': [-100, 1, 2, -100]}"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets['train'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3Ffx4IE_lUD",
        "outputId": "58a569ba-7bd9-4dea-9026-1fb72f847b86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Ru-b_XOZ_lW8"
      },
      "outputs": [],
      "source": [
        "# Define training args\n",
        "args = TrainingArguments(\n",
        "  \"test-ner\",\n",
        "  eval_strategy = \"epoch\",\n",
        "  learning_rate=2e-5,\n",
        "  per_device_train_batch_size=16,\n",
        "  per_device_eval_batch_size=16,\n",
        "  num_train_epochs=3,\n",
        "  weight_decay=0.01,\n",
        "  report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "PqRPFn6o_laW"
      },
      "outputs": [],
      "source": [
        " data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "P7NTGEyw_lc8"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"seqeval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WZEmFH_EPn5"
      },
      "source": [
        "### test the metrix on an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "3XmFS4ps_lgK"
      },
      "outputs": [],
      "source": [
        "example = conll2003['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLjiyglS_li-",
        "outputId": "62968742-5f1c-4a16-b894-f4c7f8404499"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_list = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSiBlhxkEXyi",
        "outputId": "123102ba-125c-4d81-cf3d-6a18084a6711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "0\n",
            "7\n",
            "0\n",
            "0\n",
            "0\n",
            "7\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "for i in example[\"ner_tags\"]:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpQ0qGFrEX1O",
        "outputId": "9511428e-b0e4-4c37-cc9d-482321e38490"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsYOn_edEX4H",
        "outputId": "beeb5ec1-c40b-43e5-85fa-c31f02c977f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'MISC': {'precision': np.float64(1.0),\n",
              "  'recall': np.float64(1.0),\n",
              "  'f1': np.float64(1.0),\n",
              "  'number': np.int64(2)},\n",
              " 'ORG': {'precision': np.float64(1.0),\n",
              "  'recall': np.float64(1.0),\n",
              "  'f1': np.float64(1.0),\n",
              "  'number': np.int64(1)},\n",
              " 'overall_precision': np.float64(1.0),\n",
              " 'overall_recall': np.float64(1.0),\n",
              " 'overall_f1': np.float64(1.0),\n",
              " 'overall_accuracy': 1.0}"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric.compute(predictions=[labels], references=[labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "vd7yGqpGEX7d"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    pred_logits, labels = eval_preds\n",
        "\n",
        "    pred_logits = np.argmax(pred_logits, axis=2)\n",
        "    # the logits and the probabilities are in the same order,\n",
        "    # so we don’t need to apply the softmax\n",
        "\n",
        "    # We remove all the values where the label is -100\n",
        "    predictions = [\n",
        "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(pred_logits, labels)\n",
        "    ]\n",
        "\n",
        "    true_labels = [\n",
        "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
        "       for prediction, label in zip(pred_logits, labels)\n",
        "   ]\n",
        "    results = metric.compute(predictions=predictions, references=true_labels)\n",
        "\n",
        "    return {\n",
        "          \"precision\": results[\"overall_precision\"],\n",
        "          \"recall\": results[\"overall_recall\"],\n",
        "          \"f1\": results[\"overall_f1\"],\n",
        "          \"accuracy\": results[\"overall_accuracy\"],\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1a2cA58EjqS"
      },
      "source": [
        "### Train The model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "DcgPoFjbEYCD"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "  model,\n",
        "  args,\n",
        "  train_dataset = tokenized_datasets[\"train\"],\n",
        "  eval_dataset = tokenized_datasets[\"validation\"],\n",
        "  data_collator = data_collator,\n",
        "  tokenizer = tokenizer,\n",
        "  compute_metrics = compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "xNdNQ-FNEYFK",
        "outputId": "9f70a4cf-9fcb-4324-f40c-71f5b1460583"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2634/2634 08:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.228900</td>\n",
              "      <td>0.060095</td>\n",
              "      <td>0.922829</td>\n",
              "      <td>0.929746</td>\n",
              "      <td>0.926275</td>\n",
              "      <td>0.983160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.045600</td>\n",
              "      <td>0.056970</td>\n",
              "      <td>0.933208</td>\n",
              "      <td>0.942499</td>\n",
              "      <td>0.937830</td>\n",
              "      <td>0.985146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.026300</td>\n",
              "      <td>0.057682</td>\n",
              "      <td>0.936677</td>\n",
              "      <td>0.946526</td>\n",
              "      <td>0.941576</td>\n",
              "      <td>0.985972</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2634, training_loss=0.07834169334232038, metrics={'train_runtime': 503.037, 'train_samples_per_second': 83.737, 'train_steps_per_second': 5.236, 'total_flos': 1020143109346326.0, 'train_loss': 0.07834169334232038, 'epoch': 3.0})"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "IsTfAqPJbKQ2"
      },
      "outputs": [],
      "source": [
        "# Save model locally\n",
        "model.save_pretrained(\"ner_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNl_YcQsJpO1",
        "outputId": "4e261c90-5918-4a45-871c-1905ac5f61b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('tokenizer/tokenizer_config.json',\n",
              " 'tokenizer/special_tokens_map.json',\n",
              " 'tokenizer/vocab.txt',\n",
              " 'tokenizer/added_tokens.json',\n",
              " 'tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "bRamDiCYOZ-2"
      },
      "outputs": [],
      "source": [
        "id2label = {\n",
        "    str(i): label for i,label in enumerate(label_list)\n",
        "}\n",
        "label2id = {\n",
        "    label: str(i) for i,label in enumerate(label_list)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lfbo0phOavo",
        "outputId": "df069d39-8f8b-4ca3-ce79-b6ea42f01417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'0': 'O', '1': 'B-PER', '2': 'I-PER', '3': 'B-ORG', '4': 'I-ORG', '5': 'B-LOC', '6': 'I-LOC', '7': 'B-MISC', '8': 'I-MISC'}\n",
            "{'O': '0', 'B-PER': '1', 'I-PER': '2', 'B-ORG': '3', 'I-ORG': '4', 'B-LOC': '5', 'I-LOC': '6', 'B-MISC': '7', 'I-MISC': '8'}\n"
          ]
        }
      ],
      "source": [
        "print(id2label)\n",
        "print(label2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnpaCCrEOCTq"
      },
      "source": [
        "## Loading model & prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "LXnbxb3rJsoF"
      },
      "outputs": [],
      "source": [
        "config = json.load(open(\"ner_model/config.json\"))\n",
        "config[\"id2label\"] = id2label\n",
        "config[\"label2id\"] = label2id\n",
        "json.dump(config, open(\"ner_model/config.json\",\"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "3fbpjdx4Qqxn"
      },
      "outputs": [],
      "source": [
        "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wzBg5EaQq06",
        "outputId": "4d76ab87-1273-492a-92da-57b191eedcd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'entity': 'B-PER', 'score': np.float32(0.991267), 'index': 1, 'word': 'bill', 'start': 0, 'end': 4}, {'entity': 'I-PER', 'score': np.float32(0.9903058), 'index': 2, 'word': 'gates', 'start': 5, 'end': 10}, {'entity': 'B-ORG', 'score': np.float32(0.9890366), 'index': 7, 'word': 'microsoft', 'start': 29, 'end': 38}]\n"
          ]
        }
      ],
      "source": [
        "nlp = pipeline(\"ner\", model=model_fine_tuned, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "example = \"Bill Gates is the Founder of Microsoft\"\n",
        "\n",
        "ner_results = nlp(example)\n",
        "\n",
        "print(ner_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxjmDw-HRwss"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
